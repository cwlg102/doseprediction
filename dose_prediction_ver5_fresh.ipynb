{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai; import nibabel; import tqdm\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21번과 비교실험 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import functools\n",
    "from torch.nn import init\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.losses.ssim_loss import SSIMLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    RandGaussianNoise,\n",
    "    RandGaussianSmooth,\n",
    "    RandZoomd,\n",
    "    RandFlip,\n",
    "    RandRotate90,\n",
    "    RandAdjustContrast,\n",
    "    RandShiftIntensity,\n",
    "    RandGibbsNoise,\n",
    "    ScaleIntensity,\n",
    "    RandSimulateLowResolutiond\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import MAEMetric\n",
    "from monai.networks.nets import (SwinUNETR, UNETR, UNet, DynUNet, SegResNet)\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SingleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, padding):\n",
    "        super(SingleConv, self).__init__()\n",
    "\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=kernel_size, padding=padding, stride=stride, bias=True),\n",
    "            nn.InstanceNorm3d(out_ch, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.single_conv(x)\n",
    "\n",
    "\n",
    "class DenseConvolve(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate=16, stride=(1, 1, 1)):\n",
    "        super(DenseConvolve, self).__init__()\n",
    "\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, growth_rate, kernel_size=(3, 3, 3), padding=1, stride=stride, bias=True),\n",
    "            nn.InstanceNorm3d(growth_rate, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat((self.single_conv(x), x), dim=1)\n",
    "\n",
    "\n",
    "class DenseDownsample(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate=16, stride=(2, 2, 2)):\n",
    "        super(DenseDownsample, self).__init__()\n",
    "\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, growth_rate, kernel_size=(3, 3, 3), padding=1, stride=stride, bias=True),\n",
    "            nn.InstanceNorm3d(growth_rate, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.pooling = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat((self.single_conv(x), self.pooling(x)), dim=1)\n",
    "\n",
    "\n",
    "class UNetUpsample(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UNetUpsample, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=(3, 3, 3), padding=1, stride=(1, 1, 1), bias=True),\n",
    "            nn.InstanceNorm3d(out_ch, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_1 = nn.Sequential(\n",
    "            DenseConvolve(in_ch, growth_rate),\n",
    "            DenseConvolve(in_ch + growth_rate, growth_rate),\n",
    "        )\n",
    "        self.encoder_2 = nn.Sequential(\n",
    "            DenseDownsample(in_ch + 2 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 3 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 4 * growth_rate, growth_rate)\n",
    "        )\n",
    "        self.encoder_3 = nn.Sequential(\n",
    "            DenseDownsample(in_ch + 5 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 6 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 7 * growth_rate, growth_rate)\n",
    "        )\n",
    "        self.encoder_4 = nn.Sequential(\n",
    "            DenseDownsample(in_ch + 8 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 9 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 10 * growth_rate, growth_rate)\n",
    "        )\n",
    "        self.encoder_5 = nn.Sequential(\n",
    "            DenseDownsample(in_ch + 11 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 12 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 13 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 14 * growth_rate, growth_rate),\n",
    "            DenseConvolve(in_ch + 15 * growth_rate, growth_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder_1 = self.encoder_1(x)\n",
    "        out_encoder_2 = self.encoder_2(out_encoder_1)\n",
    "        out_encoder_3 = self.encoder_3(out_encoder_2)\n",
    "        out_encoder_4 = self.encoder_4(out_encoder_3)\n",
    "        out_encoder_5 = self.encoder_5(out_encoder_4)\n",
    "\n",
    "        return [out_encoder_1, out_encoder_2, out_encoder_3, out_encoder_4, out_encoder_5]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate, upsample_chan):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.upconv_4 = UNetUpsample(in_ch + 16 * growth_rate, upsample_chan)\n",
    "        self.decoder_conv_4 = nn.Sequential(\n",
    "            SingleConv(in_ch + 11 * growth_rate + upsample_chan, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1),\n",
    "                       padding=1),\n",
    "            SingleConv(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1)\n",
    "        )\n",
    "        self.upconv_3 = UNetUpsample(256, upsample_chan)\n",
    "        self.decoder_conv_3 = nn.Sequential(\n",
    "            SingleConv(in_ch + 8 * growth_rate + upsample_chan, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1),\n",
    "                       padding=1),\n",
    "            SingleConv(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1)\n",
    "        )\n",
    "        self.upconv_2 = UNetUpsample(128, upsample_chan)\n",
    "        self.decoder_conv_2 = nn.Sequential(\n",
    "            SingleConv(in_ch + 5 * growth_rate + upsample_chan, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1),\n",
    "            SingleConv(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1)\n",
    "        )\n",
    "        self.upconv_1 = UNetUpsample(64, upsample_chan)\n",
    "        self.decoder_conv_1 = nn.Sequential(\n",
    "            SingleConv(in_ch + 2 * growth_rate + upsample_chan, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1),\n",
    "            SingleConv(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1)\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=True)\n",
    "\n",
    "    def forward(self, out_encoder):\n",
    "        out_encoder_1, out_encoder_2, out_encoder_3, out_encoder_4, out_encoder_5 = out_encoder\n",
    "\n",
    "        out_decoder_4 = self.decoder_conv_4(\n",
    "            torch.cat((self.upconv_4(out_encoder_5), out_encoder_4), dim=1)\n",
    "        )\n",
    "        out_decoder_3 = self.decoder_conv_3(\n",
    "            torch.cat((self.upconv_3(out_decoder_4), out_encoder_3), dim=1)\n",
    "        )\n",
    "        out_decoder_2 = self.decoder_conv_2(\n",
    "            torch.cat((self.upconv_2(out_decoder_3), out_encoder_2), dim=1)\n",
    "        )\n",
    "        out_decoder_1 = self.decoder_conv_1(\n",
    "            torch.cat((self.upconv_1(out_decoder_2), out_encoder_1), dim=1)\n",
    "        )\n",
    "\n",
    "        final_output = self.final_conv(out_decoder_1)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "class HD_UNet(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate, upsample_chan):\n",
    "        super(HD_UNet, self).__init__()\n",
    "        self.encoder = Encoder(in_ch, growth_rate)\n",
    "        self.decoder = Decoder(in_ch, growth_rate, upsample_chan)\n",
    "\n",
    "        # init\n",
    "        self.initialize()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_conv_IN(modules):\n",
    "        for m in modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.InstanceNorm3d):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    def initialize(self):\n",
    "        print('# random init encoder weight using nn.init.kaiming_uniform !')\n",
    "        self.init_conv_IN(self.decoder.modules)\n",
    "        print('# random init decoder weight using nn.init.kaiming_uniform !')\n",
    "        self.init_conv_IN(self.encoder.modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder = self.encoder(x)\n",
    "        out_decoder = self.decoder(out_encoder)\n",
    "\n",
    "        # Output is a list: [Output]\n",
    "        return out_decoder\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate, upsample_chan):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.model = HD_UNet(in_ch, growth_rate, upsample_chan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm3d, use_sigmoid=False):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm3d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm3d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [\n",
    "            nn.Conv3d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv3d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv3d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv3d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "def get_kernels_strides(patch_size, spacing):\n",
    "    \"\"\"\n",
    "    This function is only used for decathlon datasets with the provided patch sizes.\n",
    "    When refering this method for other tasks, please ensure that the patch size for each spatial dimension should\n",
    "    be divisible by the product of all strides in the corresponding dimension.\n",
    "    In addition, the minimal spatial size should have at least one dimension that has twice the size of\n",
    "    the product of all strides. For patch sizes that cannot find suitable strides, an error will be raised.\n",
    "\n",
    "    \"\"\"\n",
    "    sizes, spacings = patch_size, spacing\n",
    "    input_size = sizes\n",
    "    strides, kernels = [], []\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [2 if ratio <= 2 and size >= 8 else 1 for (ratio, size) in zip(spacing_ratio, sizes)]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        for idx, (i, j) in enumerate(zip(sizes, stride)):\n",
    "            if i % j != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Patch size is not supported, please try to modify the size {input_size[idx]} in the spatial dimension {idx}.\"\n",
    "                )\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "class PolyLRScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr: float, max_steps: int, exponent: float = 0.9, current_step: int = None):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_steps = max_steps\n",
    "        self.exponent = exponent\n",
    "        self.ctr = 0\n",
    "        super().__init__(optimizer, current_step if current_step is not None else -1, False)\n",
    "\n",
    "    def step(self, current_step=None):\n",
    "        if current_step is None or current_step == -1:\n",
    "            current_step = self.ctr\n",
    "            self.ctr += 1\n",
    "\n",
    "        new_lr = self.initial_lr * (1 - current_step / self.max_steps) ** self.exponent\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(input)\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)\n",
    "    \n",
    "norm_layer = functools.partial(nn.InstanceNorm3d, affine=False, track_running_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def oar_run_train():\n",
    "    \n",
    "    root_dir = r\"C:\\! Project\\2024_unitydoseprediction\\traindata\"\n",
    "    ckpt_dir = r\"G:\\! project\\2024- UnityDosePrediction\\ckpt\"\n",
    "    data_dir = root_dir + r\"/\"\n",
    "    num_samples = 2\n",
    "    model_savepath = os.path.join(ckpt_dir, \"20240813model_contrast_hdunet\")\n",
    "    loss_savepath = os.path.join(ckpt_dir, \"20240813loss_contrast_hdunet\")\n",
    "    os.makedirs(model_savepath, exist_ok=True)\n",
    "    os.makedirs(loss_savepath, exist_ok=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    space_x, space_y, space_z = 1.1, 1.1, 2.0\n",
    "    a_min, a_max, b_min, b_max = -175, 350, 0, 1\n",
    "    spatial_size_xyz = (96, 96, 64)\n",
    "    \n",
    "    train_transforms = Compose(\n",
    "           [\n",
    "                LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "                # Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "                EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False),\n",
    "                RandRotate90d(\n",
    "                     keys=[\"image\", \"label\"],\n",
    "                     prob=0.2,\n",
    "                     max_k=3,\n",
    "                     spatial_axes=(0, 1)\n",
    "                ),\n",
    "                \n",
    "                # RandZoomd(\n",
    "                #     keys=[\"image\", \"label\"],\n",
    "                #     min_zoom=0.7,\n",
    "                #     max_zoom=1.4,\n",
    "                #     mode=(\"trilinear\", \"nearest\"),\n",
    "                #     align_corners=(True, None),\n",
    "                #     prob=0.20,\n",
    "                # ),\n",
    "                RandFlipd([\"image\", \"label\"], spatial_axis=[0], prob=0.3),\n",
    "                RandFlipd([\"image\", \"label\"], spatial_axis=[1], prob=0.3),\n",
    "                RandFlipd([\"image\", \"label\"], spatial_axis=[2], prob=0.3),\n",
    "\n",
    "                # RandCropByPosNegLabeld(\n",
    "                #     keys=[\"image\", \"label\"],\n",
    "                #     label_key=\"label\",\n",
    "                #     spatial_size=spatial_size_xyz,\n",
    "                #     pos=1,\n",
    "                #     neg=1,\n",
    "                #     num_samples=num_samples,\n",
    "                #     image_key=\"image\",\n",
    "                #     image_threshold=0,\n",
    "                # ),\n",
    "                RandSpatialCropSamplesd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    roi_size=spatial_size_xyz,\n",
    "                    num_samples=num_samples,\n",
    "                    random_size=False\n",
    "                ),\n",
    "                RandSimulateLowResolutiond(keys=[\"image\"], prob=0.2),\n",
    "                # RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.1),\n",
    "                # RandGaussianSmoothd(\n",
    "                #     keys=[\"image\"],\n",
    "                #     sigma_x=(0.5, 1.15),\n",
    "                #     sigma_y=(0.5, 1.15),\n",
    "                #     sigma_z=(0.5, 1.15),\n",
    "                #     prob=0.2    \n",
    "                # ),\n",
    "                \n",
    "                # RandRotate90d(\n",
    "                #      keys=[\"image\", \"label\"],\n",
    "                #      prob=0.3,\n",
    "                #      max_k=3,\n",
    "                # )\n",
    "                # RandShiftIntensityd(\n",
    "                #     keys=[\"image\"],\n",
    "                #     offsets=0.10,\n",
    "                #     prob=0.50,\n",
    "                # ),\n",
    "            ]\n",
    "            ) \n",
    "    \n",
    "        \n",
    "    \n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "            # ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "            # Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "            EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "            ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "            Spacingd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                pixdim=(space_x, space_y, space_z),\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "            EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    split_json = \"dataset_f0\" + \".json\"\n",
    "    \n",
    "    datasets = data_dir + split_json\n",
    "    datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "    val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "    train_ds = CacheDataset(\n",
    "        data=datalist,\n",
    "        transform=train_transforms,\n",
    "        cache_num=45,\n",
    "        cache_rate=1.0,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    train_loader = ThreadDataLoader(train_ds, num_workers=0, batch_size=1, shuffle=True)\n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=0, cache_rate=1.0, num_workers=4)\n",
    "    val_loader = ThreadDataLoader(val_ds, num_workers=0, batch_size=1)\n",
    "\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    OAR_nums_plus_one = 1\n",
    "    patch_size = list(spatial_size_xyz)\n",
    "    spacing = [3.0, 3.0, 3.0]\n",
    "    # # ks, st = get_kernels_strides(patch_size, spacing)\n",
    "    # # print(ks, st)\n",
    "    # # uks = st[1:]\n",
    "    #dynunet hyperparameter\n",
    "   \n",
    "    # ks = [[3, 3, 1], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n",
    "    # st = [[1, 1, 1], [2, 2, 1], [2, 2, 2], [2, 2, 1]]\n",
    "    # uks = st[1:]\n",
    "\n",
    "    # netG = ResnetGenerator(12, 1, 64, norm_layer, False, 6, \"reflect\").to(device)\n",
    "    netG = Model(in_ch=12, growth_rate=16, upsample_chan=64).to(device)\n",
    "    # netD = NLayerDiscriminator(13, 64, 3, norm_layer, False).to(device)\n",
    "    \n",
    "    # model = SegResNet(in_channels = 5, out_channels = OAR_nums_plus_one, dropout_prob = 0.3, act=\"LEAKYRELU\").to(device)\n",
    "    # model = DynUNet(\n",
    "    #     spatial_dims=3,\n",
    "    #     in_channels=12,\n",
    "    #     out_channels=OAR_nums_plus_one,\n",
    "    #     kernel_size=ks,\n",
    "    #     strides=st,\n",
    "    #     upsample_kernel_size=uks,\n",
    "    #     dropout=0.1,\n",
    "    #     act_name= \"LEAKYRELU\",\n",
    "    #     deep_supervision=False\n",
    "    # ).to(device)\n",
    "    \n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #FOR SINGLE CHANNEL PREDICTION\n",
    "    # loss_function = nn.L1Loss()\n",
    "    loss_function = nn.MSELoss()\n",
    "    # criterionGAN = GANLoss().to(device)\n",
    "    initial_lr = 1e-4\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=3e-5, momentum=0.99, nesterov=True)\n",
    "    optimizer_G = torch.optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    # optimizer_D = torch.optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    # scheduler = CosineAnnealingNoWarmUpRestarts(optimizer, 5, 2, 1e-2, 0, 0, 0.8)\n",
    "    T_0 = 20\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=2, eta_min=0)\n",
    "    scheduler_G = PolyLRScheduler(optimizer_G, initial_lr=initial_lr, max_steps=500)\n",
    "    # scheduler_D = PolyLRScheduler(optimizer_D, initial_lr=initial_lr, max_steps=500)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # ckpt = torch.load(r\"G:\\! project\\2024- UnityDosePrediction\\ckpt\\20240813model_contrast_hdunet\\doseprediction_modelG_70.pth\")\n",
    "    \n",
    "    # netG.load_state_dict(ckpt[\"netG_state_dict\"])\n",
    "    # optimizer_G.load_state_dict(ckpt[\"optimizer_G_state_dict\"])\n",
    "    # scheduler_G.load_state_dict(ckpt[\"scheduler_G_state_dict\"])\n",
    "    \n",
    "    import gc\n",
    "    import random\n",
    "    def validation(epoch_iterator_val):\n",
    "        netG.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in epoch_iterator_val:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "                \n",
    "                # tx =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "                \n",
    "                # val_inputs = torch.cat((tx, val_inputs[:, 1:, :, :, :]), 1)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    val_outputs = sliding_window_inference(val_inputs, spatial_size_xyz, 4, netG)\n",
    "                # val_labels_list = decollate_batch(val_labels)\n",
    "                # val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "                # val_outputs_list = decollate_batch(val_outputs)\n",
    "                # val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                MAE_metric(y_pred=val_outputs, y=val_labels)\n",
    "                epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))\n",
    "            mean_mae_val = MAE_metric.aggregate().item()\n",
    "            MAE_metric.reset()\n",
    "        return mean_mae_val\n",
    "\n",
    "\n",
    "    def train(global_step, train_loader, mae_val_best, global_step_best, t_0):\n",
    "        netG.train()\n",
    "        # netD.train()\n",
    "        \n",
    "        randcontrast = RandAdjustContrast(prob=0.1, gamma=(0.7, 1.5))\n",
    "        randintensity = RandShiftIntensity(prob=0.1, offsets=0.4)\n",
    "        randgibbs = RandGibbsNoise(prob=0.2, alpha=(0.5, 0.6))\n",
    "        rgsmooth = RandGaussianSmooth(sigma_x=(0.5, 1.15),\n",
    "                    sigma_y=(0.5, 1.15),\n",
    "                    sigma_z=(0.5, 1.15),\n",
    "                    prob=0.1)\n",
    "        rgnoise = RandGaussianNoise(prob=0.1, std=0.1)\n",
    "        epoch_loss_D = 0\n",
    "        epoch_loss_G = 0\n",
    "        step = 0\n",
    "        # rand_fliper_x = RandFlip(prob=0.5, spatial_axis=0)\n",
    "        # rand_fliper_y = RandFlip(prob=0.1, spatial_axis=1)\n",
    "        # rand_fliper_z = RandFlip(prob=0.1, spatial_axis=2)\n",
    "        size_x = spatial_size_xyz[0]\n",
    "        size_y = spatial_size_xyz[1]\n",
    "        size_z = spatial_size_xyz[2]\n",
    "        \n",
    "        epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "        scale_intensity = ScaleIntensity(minv=-1, maxv=1)\n",
    "        # from PIL import Image\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            step += 1\n",
    "            realA, realB = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "            # now_x = realA.shape[2]\n",
    "            # now_y = realA.shape[3]\n",
    "            # now_z = realA.shape[4]\n",
    "            # max_x = now_x - size_x\n",
    "            # max_y = now_y - size_y \n",
    "            # max_z = now_z - size_z \n",
    "            # rnd_x = random.randint(0, max_x)\n",
    "            # rnd_y = random.randint(0, max_y)\n",
    "            # rnd_z = random.randint(0, max_z)\n",
    "            # realA = realA[:, :, rnd_x:rnd_x+size_x, rnd_y:rnd_y+size_y, rnd_z:rnd_z+size_z].cuda()\n",
    "            # realB = realB[:, :, rnd_x:rnd_x+size_x, rnd_y:rnd_y+size_y, rnd_z:rnd_z+size_z].cuda()\n",
    "            \n",
    "            tx =  torch.reshape(realA[:, 0, :, : ,:].clone().detach(), (realA.shape[0], 1, realA.shape[2], realA.shape[3], realA.shape[4]))\n",
    "            one = torch.reshape(realA[:, 1, :, : ,:].clone().detach(), (realA.shape[0], 1, realA.shape[2], realA.shape[3], realA.shape[4]))\n",
    "            ptv = torch.reshape(realA[:, 2, :, : ,:].clone().detach(), (realA.shape[0], 1, realA.shape[2], realA.shape[3], realA.shape[4]))\n",
    "            tx = scale_intensity(tx)\n",
    "            ptv = scale_intensity(ptv)\n",
    "            realB = scale_intensity(realB)\n",
    "            # mr_img = np.squeeze(tx.detach().clone().cpu().numpy()[0])\n",
    "            # ptv_img = np.squeeze(ptv.detach().clone().cpu().numpy()[0])\n",
    "            # realB_img = np.squeeze(realB.detach().clone().cpu().numpy()[0])\n",
    "            \n",
    "            # mr_img = np.uint8( 255 * (np.transpose(mr_img, (2, 1, 0)) - np.min(mr_img))/(np.max(mr_img) - np.min(mr_img)))\n",
    "            # ptv_img = np.uint8( 255 * (np.transpose(ptv_img, (2, 1, 0)) - np.min(ptv_img))/(np.max(ptv_img) - np.min(ptv_img)))\n",
    "            # realB_img = np.uint8( 255 * (np.transpose(realB_img, (2, 1, 0)) - np.min(realB_img))/(np.max(realB_img) - np.min(realB_img)))\n",
    "            # img = np.concatenate((mr_img, ptv_img, realB_img), axis = 2)\n",
    "            # for i in range(len(img)):\n",
    "            #     IM = Image.fromarray(img[i])\n",
    "            #     IM.save(os.path.join(r\"G:\\! project\\2024- UnityDosePrediction\\debug\", \"P%06d_%04d_%03d.png\" %(global_step,step, i)))\n",
    "            \n",
    "            \n",
    "            # tx = randgibbs(tx)\n",
    "            # tx = randintensity(tx)\n",
    "            # tx = randcontrast(tx)& my\n",
    "            # tx = rgsmooth(tx)\n",
    "            # tx = rgnoise(tx)\n",
    "\n",
    "            realA = torch.cat((tx, one, ptv, realA[:, 3:, :, :, :]), 1)\n",
    "            # realB = scale_intensity(realB)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                fakeB = netG(realA)\n",
    "                # print(len(fakeB))\n",
    "                # realAB = torch.cat((realA, realB), dim = 1)\n",
    "                # fakeAB = torch.cat((realA, fakeB), dim = 1)\n",
    "                # pred_fake = netD(fakeAB.detach())\n",
    "                # pred_real = netD(realAB.detach())\n",
    "                # loss_D_fake = criterionGAN(pred_fake, False)\n",
    "                # loss_D_real = criterionGAN(pred_real, True)\n",
    "                # loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "                \n",
    "                # pred_G_fake = netD(fakeAB)\n",
    "                # loss_G_GAN = criterionGAN(pred_G_fake, True)\n",
    "                L1_loss = loss_function(fakeB, realB)\n",
    "               \n",
    "                 \n",
    "\n",
    "            # scaler.scale(loss_D).backward()\n",
    "            # epoch_loss_D += loss_D.item()\n",
    "            # scaler.unscale_(optimizer_D)\n",
    "            # scaler.step(optimizer_D)\n",
    "            # optimizer_D.zero_grad()\n",
    "            \n",
    "            scaler.scale(L1_loss).backward()\n",
    "            epoch_loss_G += L1_loss.item()\n",
    "            scaler.unscale_(optimizer_G)\n",
    "            scaler.step(optimizer_G)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            scaler.update()\n",
    "            \n",
    "            \n",
    "            epoch_iterator.set_description(f\"Training ({global_step} / {max_iterations} Steps) (L2_loss={L1_loss:2.5f})\")\n",
    "            if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
    "                if (global_step // eval_num) % save_period == 0:\n",
    "                    global_step_best = global_step\n",
    "                    torch.save({\n",
    "                        'global_step': global_step,\n",
    "                        'netG_state_dict': netG.state_dict(),\n",
    "                        'optimizer_G_state_dict' : optimizer_G.state_dict(),\n",
    "                        'scheduler_G_state_dict': scheduler_G.state_dict(),\n",
    "                    }, os.path.join(model_savepath, \"doseprediction_modelG_%d.pth\" %(global_step//eval_num)))\n",
    "                    # torch.save({\n",
    "                    # 'global_step': global_step,\n",
    "                    # 'netD_state_dict': netD.state_dict(),\n",
    "                    # 'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                    # 'scheduler_D_state_dict': scheduler_D.state_dict()\n",
    "                    # }, os.path.join(model_savepath, \"doseprediction_modelD_%d.pth\" %(global_step//eval_num)))\n",
    "                    \n",
    "                    # print(optimizer.param_groups[0]['lr'])\n",
    "                    scheduler_G.step()\n",
    "                    # scheduler_D.step()\n",
    "            global_step += 1\n",
    "        # print((global_step+1) // (len(train_ds)) % t_0)\n",
    "        # if (global_step+1) // (len(train_ds)) % t_0 == 0:\n",
    "        #     print(\"donedone\")\n",
    "        #     scheduler.base_lrs[0] = scheduler.base_lrs[0] * (0.7)\n",
    "        #     t_0 *= 2\n",
    "        \n",
    "        return global_step, mae_val_best, global_step_best, t_0\n",
    "     \n",
    "    max_iterations = 62503\n",
    "    eval_num = 125\n",
    "    post_label = AsDiscrete(to_onehot=OAR_nums_plus_one)\n",
    "    post_pred = AsDiscrete(argmax=True, to_onehot=OAR_nums_plus_one)\n",
    "    MAE_metric = MAEMetric(reduction=\"mean\", get_not_nans=False)\n",
    "    global_step = 0\n",
    "    mae_val_best = 1000.0\n",
    "    global_step_best = 0\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    save_period = 10\n",
    "    t_0 = T_0\n",
    "    # global_step = ckpt[\"global_step\"]\n",
    "    while global_step < max_iterations:\n",
    "        global_step, mae_val_best, global_step_best, t_0 = train(global_step, train_loader, mae_val_best, global_step_best, t_0=t_0)\n",
    "        epoch_loss_npy = np.array(epoch_loss_values)\n",
    "        np.save(os.path.join(loss_savepath, \"%d_loss.npy\" %(int(global_step))), epoch_loss_npy)\n",
    "    # total_case_num = 24\n",
    "    # model.load_state_dict(torch.load(os.path.join(root_dir, name_oar.lower() + \"_model_fold0_0.pth\")))\n",
    "    # model.eval()\n",
    "    # original_nib_path = root_dir + r\"/imcroppedval\"\n",
    "    # original_nib_path_list = os.listdir(original_nib_path)\n",
    "    return None\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for case_num in range(total_case_num):\n",
    "    #         start = time.time()\n",
    "    #         template_nib = nib.load(os.path.join(original_nib_path, original_nib_path_list[case_num]))\n",
    "\n",
    "    #         img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "    #         img = val_ds[case_num][\"image\"]\n",
    "    #         label = val_ds[case_num][\"label\"]\n",
    "    #         val_inputs = torch.unsqueeze(img, 0).cuda()\n",
    "    #         val_labels = torch.unsqueeze(label, 0).cuda()\n",
    "    #         bone_min = -1000\n",
    "    #         bone_max = 2000\n",
    "    #         soft_min = -160\n",
    "    #         soft_max = 350\n",
    "    #         brain_min = -5\n",
    "    #         brain_max = 65\n",
    "    #         stroke_min = 15\n",
    "    #         stroke_max = 45\n",
    "\n",
    "    #         # box_start, box_end = FG_cropper.compute_bounding_box(val_inputs)\n",
    "    #         tx =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x1 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x2 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x3 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x4 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         tx2 = torch.reshape(val_inputs[:, 1, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x1[x1<bone_min] = bone_min\n",
    "    #         x1[x1>bone_max] = bone_max\n",
    "    #         x1 = (x1-bone_min)/(bone_max-bone_min)\n",
    "    #         x2[x2<soft_min] = soft_min\n",
    "    #         x2[x2>soft_max] = soft_max\n",
    "    #         x2 = (x2-soft_min)/(soft_max-soft_min)\n",
    "    #         x3[x3<brain_min] = brain_min\n",
    "    #         x3[x3>brain_max] = brain_max \n",
    "    #         x3 = (x3-brain_min)/(brain_max-brain_min)\n",
    "    #         x4[x4<stroke_min] = stroke_min\n",
    "    #         x4[x4>stroke_max] = stroke_max\n",
    "    #         x4 = (x4 - stroke_min)/(stroke_max - stroke_min)\n",
    "    #         val_inputs = torch.cat((tx, x1, x2, x3, x4, tx2), 1)\n",
    "\n",
    "    #         val_outputs = sliding_window_inference(val_inputs, spatial_size_xyz, 4, model, overlap=0.5)\n",
    "    #         last_outputs = torch.argmax(val_outputs, dim=1).detach().cpu()[0].numpy()\n",
    "    #         img_npy = img.cpu()[0].numpy()\n",
    "    #         label_npy = label.cpu()[0].numpy()\n",
    "    #         print(\"time taken : %f\" %(time.time()- start))\n",
    "    #         # nib.save(\n",
    "    #                 # nib.Nifti1Image(img_npy.astype(np.uint8), np.ones((4, 4))), os.path.join(r\"D:\\!HaN_Challenge\\HaN-Seg_NRRD\\!output_dir\\label_1to10_fold0\", \"val_ct_%02d\" %(case_num+1))\n",
    "    #             # )\n",
    "\n",
    "    #         nib.save(\n",
    "    #                 nib.Nifti1Image(label_npy.astype(np.uint8), template_nib.affine, template_nib.header), os.path.join(root_dir + \"/result\", \"val_label_%02d\" %(case_num+1))\n",
    "    #             )\n",
    "    #         nib.save(\n",
    "    #                 nib.Nifti1Image(last_outputs.astype(np.uint8), template_nib.affine, template_nib.header), os.path.join(root_dir + \"/result\", \"infered_label_%02d\" %(case_num+1))\n",
    "    #             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 45/45 [00:08<00:00,  5.13it/s]\n",
      "c:\\Users\\Desktop\\anaconda3\\envs\\btcv_copy\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# random init encoder weight using nn.init.kaiming_uniform !\n",
      "# random init decoder weight using nn.init.kaiming_uniform !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (8794 / 62503 Steps) (L2_loss=0.00195): 100%|██████████| 45/45 [00:10<00:00,  4.46it/s]\n",
      "Training (8839 / 62503 Steps) (L2_loss=0.00237): 100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
      "Training (8884 / 62503 Steps) (L2_loss=0.00865): 100%|██████████| 45/45 [00:06<00:00,  6.85it/s]\n",
      "Training (8929 / 62503 Steps) (L2_loss=0.00695): 100%|██████████| 45/45 [00:06<00:00,  6.83it/s]\n",
      "Training (8974 / 62503 Steps) (L2_loss=0.00295): 100%|██████████| 45/45 [00:06<00:00,  6.71it/s]\n",
      "Training (9019 / 62503 Steps) (L2_loss=0.00269): 100%|██████████| 45/45 [00:06<00:00,  6.65it/s]\n",
      "Training (9064 / 62503 Steps) (L2_loss=0.00552): 100%|██████████| 45/45 [00:06<00:00,  6.59it/s]\n",
      "Training (9109 / 62503 Steps) (L2_loss=0.00317): 100%|██████████| 45/45 [00:06<00:00,  6.65it/s]\n",
      "Training (9154 / 62503 Steps) (L2_loss=0.00436): 100%|██████████| 45/45 [00:06<00:00,  6.66it/s]\n",
      "Training (9199 / 62503 Steps) (L2_loss=0.00619): 100%|██████████| 45/45 [00:06<00:00,  6.58it/s]\n",
      "Training (9244 / 62503 Steps) (L2_loss=0.00223): 100%|██████████| 45/45 [00:06<00:00,  6.52it/s]\n",
      "Training (9289 / 62503 Steps) (L2_loss=0.00644): 100%|██████████| 45/45 [00:06<00:00,  6.66it/s]\n",
      "Training (9334 / 62503 Steps) (L2_loss=0.00314): 100%|██████████| 45/45 [00:06<00:00,  6.69it/s]\n",
      "Training (9379 / 62503 Steps) (L2_loss=0.00727): 100%|██████████| 45/45 [00:06<00:00,  6.60it/s]\n",
      "Training (9424 / 62503 Steps) (L2_loss=0.00262): 100%|██████████| 45/45 [00:06<00:00,  6.63it/s]\n",
      "Training (9469 / 62503 Steps) (L2_loss=0.00164): 100%|██████████| 45/45 [00:06<00:00,  6.63it/s]\n",
      "Training (9514 / 62503 Steps) (L2_loss=0.00211): 100%|██████████| 45/45 [00:06<00:00,  6.66it/s]\n",
      "Training (9559 / 62503 Steps) (L2_loss=0.00470): 100%|██████████| 45/45 [00:06<00:00,  6.62it/s]\n",
      "Training (9604 / 62503 Steps) (L2_loss=0.00335): 100%|██████████| 45/45 [00:06<00:00,  6.64it/s]\n",
      "Training (9649 / 62503 Steps) (L2_loss=0.00176): 100%|██████████| 45/45 [00:06<00:00,  6.59it/s]\n",
      "Training (9694 / 62503 Steps) (L2_loss=0.00475): 100%|██████████| 45/45 [00:06<00:00,  6.60it/s]\n",
      "Training (9739 / 62503 Steps) (L2_loss=0.00381): 100%|██████████| 45/45 [00:06<00:00,  6.51it/s]\n",
      "Training (9784 / 62503 Steps) (L2_loss=0.00184): 100%|██████████| 45/45 [00:06<00:00,  6.70it/s]\n",
      "Training (9829 / 62503 Steps) (L2_loss=0.00399): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (9874 / 62503 Steps) (L2_loss=0.00175): 100%|██████████| 45/45 [00:06<00:00,  6.62it/s]\n",
      "Training (9919 / 62503 Steps) (L2_loss=0.00291): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (9964 / 62503 Steps) (L2_loss=0.00601): 100%|██████████| 45/45 [00:06<00:00,  6.73it/s]\n",
      "Training (10009 / 62503 Steps) (L2_loss=0.00582): 100%|██████████| 45/45 [00:06<00:00,  6.61it/s]\n",
      "Training (10054 / 62503 Steps) (L2_loss=0.00367): 100%|██████████| 45/45 [00:06<00:00,  6.74it/s]\n",
      "Training (10099 / 62503 Steps) (L2_loss=0.00176): 100%|██████████| 45/45 [00:06<00:00,  6.77it/s]\n",
      "Training (10144 / 62503 Steps) (L2_loss=0.00168): 100%|██████████| 45/45 [00:06<00:00,  6.76it/s]\n",
      "Training (10189 / 62503 Steps) (L2_loss=0.00289): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (10234 / 62503 Steps) (L2_loss=0.00254): 100%|██████████| 45/45 [00:06<00:00,  6.71it/s]\n",
      "Training (10279 / 62503 Steps) (L2_loss=0.00213): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (10324 / 62503 Steps) (L2_loss=0.00160): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (10369 / 62503 Steps) (L2_loss=0.00161): 100%|██████████| 45/45 [00:06<00:00,  6.73it/s]\n",
      "Training (10414 / 62503 Steps) (L2_loss=0.00138): 100%|██████████| 45/45 [00:06<00:00,  6.74it/s]\n",
      "Training (10459 / 62503 Steps) (L2_loss=0.00233): 100%|██████████| 45/45 [00:06<00:00,  6.77it/s]\n",
      "Training (10504 / 62503 Steps) (L2_loss=0.00368): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (10549 / 62503 Steps) (L2_loss=0.00187): 100%|██████████| 45/45 [00:06<00:00,  6.64it/s]\n",
      "Training (10594 / 62503 Steps) (L2_loss=0.00494): 100%|██████████| 45/45 [00:06<00:00,  6.69it/s]\n",
      "Training (10639 / 62503 Steps) (L2_loss=0.00175): 100%|██████████| 45/45 [00:06<00:00,  7.01it/s]\n",
      "Training (10684 / 62503 Steps) (L2_loss=0.00512): 100%|██████████| 45/45 [00:06<00:00,  7.04it/s]\n",
      "Training (10729 / 62503 Steps) (L2_loss=0.00320): 100%|██████████| 45/45 [00:09<00:00,  4.98it/s]\n",
      "Training (10774 / 62503 Steps) (L2_loss=0.00508): 100%|██████████| 45/45 [00:09<00:00,  4.92it/s]\n",
      "Training (10819 / 62503 Steps) (L2_loss=0.00986): 100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
      "Training (10864 / 62503 Steps) (L2_loss=0.00214): 100%|██████████| 45/45 [00:06<00:00,  7.07it/s]\n",
      "Training (10909 / 62503 Steps) (L2_loss=0.00289): 100%|██████████| 45/45 [00:06<00:00,  7.00it/s]\n",
      "Training (10954 / 62503 Steps) (L2_loss=0.00128): 100%|██████████| 45/45 [00:06<00:00,  7.05it/s]\n",
      "Training (10999 / 62503 Steps) (L2_loss=0.00212): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (11044 / 62503 Steps) (L2_loss=0.00104): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (11089 / 62503 Steps) (L2_loss=0.00172): 100%|██████████| 45/45 [00:06<00:00,  7.07it/s]\n",
      "Training (11134 / 62503 Steps) (L2_loss=0.00269): 100%|██████████| 45/45 [00:06<00:00,  7.04it/s]\n",
      "Training (11179 / 62503 Steps) (L2_loss=0.00584): 100%|██████████| 45/45 [00:06<00:00,  6.96it/s]\n",
      "Training (11224 / 62503 Steps) (L2_loss=0.00259): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (11269 / 62503 Steps) (L2_loss=0.00152): 100%|██████████| 45/45 [00:06<00:00,  6.51it/s]\n",
      "Training (11314 / 62503 Steps) (L2_loss=0.00460): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (11359 / 62503 Steps) (L2_loss=0.00132): 100%|██████████| 45/45 [00:06<00:00,  7.13it/s]\n",
      "Training (11404 / 62503 Steps) (L2_loss=0.00370): 100%|██████████| 45/45 [00:06<00:00,  7.09it/s]\n",
      "Training (11449 / 62503 Steps) (L2_loss=0.00302): 100%|██████████| 45/45 [00:06<00:00,  6.97it/s]\n",
      "Training (11494 / 62503 Steps) (L2_loss=0.00213): 100%|██████████| 45/45 [00:06<00:00,  6.80it/s]\n",
      "Training (11539 / 62503 Steps) (L2_loss=0.00196): 100%|██████████| 45/45 [00:06<00:00,  6.82it/s]\n",
      "Training (11584 / 62503 Steps) (L2_loss=0.00547): 100%|██████████| 45/45 [00:06<00:00,  6.70it/s]\n",
      "Training (11629 / 62503 Steps) (L2_loss=0.00095): 100%|██████████| 45/45 [00:06<00:00,  6.61it/s]\n",
      "Training (11674 / 62503 Steps) (L2_loss=0.00663): 100%|██████████| 45/45 [00:06<00:00,  6.95it/s]\n",
      "Training (11719 / 62503 Steps) (L2_loss=0.00538): 100%|██████████| 45/45 [00:06<00:00,  7.10it/s]\n",
      "Training (11764 / 62503 Steps) (L2_loss=0.00396): 100%|██████████| 45/45 [00:06<00:00,  6.88it/s]\n",
      "Training (11809 / 62503 Steps) (L2_loss=0.00173): 100%|██████████| 45/45 [00:06<00:00,  6.57it/s]\n",
      "Training (11854 / 62503 Steps) (L2_loss=0.00232): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (11899 / 62503 Steps) (L2_loss=0.00363): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (11944 / 62503 Steps) (L2_loss=0.00142): 100%|██████████| 45/45 [00:06<00:00,  6.79it/s]\n",
      "Training (11989 / 62503 Steps) (L2_loss=0.00179): 100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
      "Training (12034 / 62503 Steps) (L2_loss=0.00186): 100%|██████████| 45/45 [00:06<00:00,  6.84it/s]\n",
      "Training (12079 / 62503 Steps) (L2_loss=0.00143): 100%|██████████| 45/45 [00:06<00:00,  6.83it/s]\n",
      "Training (12124 / 62503 Steps) (L2_loss=0.00510): 100%|██████████| 45/45 [00:06<00:00,  6.90it/s]\n",
      "Training (12169 / 62503 Steps) (L2_loss=0.00662): 100%|██████████| 45/45 [00:06<00:00,  6.95it/s]\n",
      "Training (12214 / 62503 Steps) (L2_loss=0.00390): 100%|██████████| 45/45 [00:06<00:00,  7.00it/s]\n",
      "Training (12259 / 62503 Steps) (L2_loss=0.00479): 100%|██████████| 45/45 [00:06<00:00,  6.96it/s]\n",
      "Training (12304 / 62503 Steps) (L2_loss=0.00140): 100%|██████████| 45/45 [00:06<00:00,  6.93it/s]\n",
      "Training (12349 / 62503 Steps) (L2_loss=0.00119): 100%|██████████| 45/45 [00:06<00:00,  6.99it/s]\n",
      "Training (12394 / 62503 Steps) (L2_loss=0.00541): 100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
      "Training (12439 / 62503 Steps) (L2_loss=0.00141): 100%|██████████| 45/45 [00:06<00:00,  7.03it/s]\n",
      "Training (12484 / 62503 Steps) (L2_loss=0.00740): 100%|██████████| 45/45 [00:06<00:00,  7.12it/s]\n",
      "Training (12529 / 62503 Steps) (L2_loss=0.00563): 100%|██████████| 45/45 [00:06<00:00,  6.78it/s]\n",
      "Training (12574 / 62503 Steps) (L2_loss=0.00200): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (12619 / 62503 Steps) (L2_loss=0.00237): 100%|██████████| 45/45 [00:06<00:00,  6.78it/s]\n",
      "Training (12664 / 62503 Steps) (L2_loss=0.00295): 100%|██████████| 45/45 [00:06<00:00,  6.92it/s]\n",
      "Training (12709 / 62503 Steps) (L2_loss=0.00143): 100%|██████████| 45/45 [00:06<00:00,  6.75it/s]\n",
      "Training (12754 / 62503 Steps) (L2_loss=0.00188): 100%|██████████| 45/45 [00:06<00:00,  6.75it/s]\n",
      "Training (12799 / 62503 Steps) (L2_loss=0.00113): 100%|██████████| 45/45 [00:06<00:00,  6.98it/s]\n",
      "Training (12844 / 62503 Steps) (L2_loss=0.00262): 100%|██████████| 45/45 [00:06<00:00,  7.04it/s]\n",
      "Training (12889 / 62503 Steps) (L2_loss=0.00105): 100%|██████████| 45/45 [00:06<00:00,  6.99it/s]\n",
      "Training (12934 / 62503 Steps) (L2_loss=0.00128): 100%|██████████| 45/45 [00:06<00:00,  6.85it/s]\n",
      "Training (12979 / 62503 Steps) (L2_loss=0.00367): 100%|██████████| 45/45 [00:06<00:00,  6.85it/s]\n",
      "Training (13024 / 62503 Steps) (L2_loss=0.00396): 100%|██████████| 45/45 [00:06<00:00,  6.89it/s]\n",
      "Training (13069 / 62503 Steps) (L2_loss=0.00180): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (13114 / 62503 Steps) (L2_loss=0.00132): 100%|██████████| 45/45 [00:06<00:00,  6.95it/s]\n",
      "Training (13159 / 62503 Steps) (L2_loss=0.00117): 100%|██████████| 45/45 [00:06<00:00,  6.49it/s]\n",
      "Training (13204 / 62503 Steps) (L2_loss=0.00397): 100%|██████████| 45/45 [00:10<00:00,  4.48it/s]\n",
      "Training (13249 / 62503 Steps) (L2_loss=0.00176): 100%|██████████| 45/45 [00:07<00:00,  6.09it/s]\n",
      "Training (13294 / 62503 Steps) (L2_loss=0.00734): 100%|██████████| 45/45 [00:06<00:00,  7.08it/s]\n",
      "Training (13339 / 62503 Steps) (L2_loss=0.00112): 100%|██████████| 45/45 [00:06<00:00,  7.08it/s]\n",
      "Training (13384 / 62503 Steps) (L2_loss=0.00112): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (13429 / 62503 Steps) (L2_loss=0.00163): 100%|██████████| 45/45 [00:06<00:00,  7.07it/s]\n",
      "Training (13474 / 62503 Steps) (L2_loss=0.00077): 100%|██████████| 45/45 [00:06<00:00,  7.00it/s]\n",
      "Training (13519 / 62503 Steps) (L2_loss=0.00169): 100%|██████████| 45/45 [00:06<00:00,  6.85it/s]\n",
      "Training (13564 / 62503 Steps) (L2_loss=0.00177): 100%|██████████| 45/45 [00:07<00:00,  6.34it/s]\n",
      "Training (13609 / 62503 Steps) (L2_loss=0.00075): 100%|██████████| 45/45 [00:06<00:00,  7.01it/s]\n",
      "Training (13654 / 62503 Steps) (L2_loss=0.00065): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (13699 / 62503 Steps) (L2_loss=0.00298): 100%|██████████| 45/45 [00:06<00:00,  7.07it/s]\n",
      "Training (13744 / 62503 Steps) (L2_loss=0.00088): 100%|██████████| 45/45 [00:06<00:00,  6.94it/s]\n",
      "Training (13789 / 62503 Steps) (L2_loss=0.00180): 100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
      "Training (13834 / 62503 Steps) (L2_loss=0.00150): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (13879 / 62503 Steps) (L2_loss=0.00228): 100%|██████████| 45/45 [00:06<00:00,  7.05it/s]\n",
      "Training (13924 / 62503 Steps) (L2_loss=0.00174): 100%|██████████| 45/45 [00:06<00:00,  7.03it/s]\n",
      "Training (13969 / 62503 Steps) (L2_loss=0.00156): 100%|██████████| 45/45 [00:06<00:00,  6.94it/s]\n",
      "Training (14014 / 62503 Steps) (L2_loss=0.00082): 100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
      "Training (14059 / 62503 Steps) (L2_loss=0.00164): 100%|██████████| 45/45 [00:07<00:00,  6.29it/s]\n",
      "Training (14104 / 62503 Steps) (L2_loss=0.00164): 100%|██████████| 45/45 [00:06<00:00,  6.98it/s]\n",
      "Training (14149 / 62503 Steps) (L2_loss=0.00239): 100%|██████████| 45/45 [00:06<00:00,  6.56it/s]\n",
      "Training (14194 / 62503 Steps) (L2_loss=0.00137): 100%|██████████| 45/45 [00:06<00:00,  6.51it/s]\n",
      "Training (14239 / 62503 Steps) (L2_loss=0.00258): 100%|██████████| 45/45 [00:06<00:00,  6.48it/s]\n",
      "Training (14284 / 62503 Steps) (L2_loss=0.00280): 100%|██████████| 45/45 [00:06<00:00,  6.87it/s]\n",
      "Training (14329 / 62503 Steps) (L2_loss=0.00366): 100%|██████████| 45/45 [00:06<00:00,  6.94it/s]\n",
      "Training (14374 / 62503 Steps) (L2_loss=0.00590): 100%|██████████| 45/45 [00:06<00:00,  6.93it/s]\n",
      "Training (14419 / 62503 Steps) (L2_loss=0.00134): 100%|██████████| 45/45 [00:06<00:00,  6.70it/s]\n",
      "Training (14464 / 62503 Steps) (L2_loss=0.00118): 100%|██████████| 45/45 [00:06<00:00,  6.56it/s]\n",
      "Training (14509 / 62503 Steps) (L2_loss=0.00291): 100%|██████████| 45/45 [00:06<00:00,  7.00it/s]\n",
      "Training (14554 / 62503 Steps) (L2_loss=0.00151): 100%|██████████| 45/45 [00:06<00:00,  6.93it/s]\n",
      "Training (14599 / 62503 Steps) (L2_loss=0.00389): 100%|██████████| 45/45 [00:06<00:00,  6.83it/s]\n",
      "Training (14644 / 62503 Steps) (L2_loss=0.00209): 100%|██████████| 45/45 [00:06<00:00,  6.98it/s]\n",
      "Training (14689 / 62503 Steps) (L2_loss=0.00133): 100%|██████████| 45/45 [00:06<00:00,  6.47it/s]\n",
      "Training (14734 / 62503 Steps) (L2_loss=0.00121): 100%|██████████| 45/45 [00:06<00:00,  7.07it/s]\n",
      "Training (14779 / 62503 Steps) (L2_loss=0.00417): 100%|██████████| 45/45 [00:06<00:00,  6.97it/s]\n",
      "Training (14824 / 62503 Steps) (L2_loss=0.00102): 100%|██████████| 45/45 [00:06<00:00,  6.96it/s]\n",
      "Training (14869 / 62503 Steps) (L2_loss=0.00228): 100%|██████████| 45/45 [00:06<00:00,  6.70it/s]\n",
      "Training (14914 / 62503 Steps) (L2_loss=0.00312): 100%|██████████| 45/45 [00:06<00:00,  6.93it/s]\n",
      "Training (14959 / 62503 Steps) (L2_loss=0.00231): 100%|██████████| 45/45 [00:09<00:00,  4.63it/s]\n",
      "Training (15004 / 62503 Steps) (L2_loss=0.01789): 100%|██████████| 45/45 [00:08<00:00,  5.03it/s]\n",
      "Training (15049 / 62503 Steps) (L2_loss=0.00109): 100%|██████████| 45/45 [00:06<00:00,  6.89it/s]\n",
      "Training (15094 / 62503 Steps) (L2_loss=0.00242): 100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
      "Training (15139 / 62503 Steps) (L2_loss=0.00127): 100%|██████████| 45/45 [00:07<00:00,  6.21it/s]\n",
      "Training (15184 / 62503 Steps) (L2_loss=0.00197): 100%|██████████| 45/45 [00:06<00:00,  6.61it/s]\n",
      "Training (15229 / 62503 Steps) (L2_loss=0.00096): 100%|██████████| 45/45 [00:06<00:00,  7.04it/s]\n",
      "Training (15274 / 62503 Steps) (L2_loss=0.00143): 100%|██████████| 45/45 [00:07<00:00,  6.08it/s]\n",
      "Training (15319 / 62503 Steps) (L2_loss=0.00084): 100%|██████████| 45/45 [00:07<00:00,  6.31it/s]\n",
      "Training (15364 / 62503 Steps) (L2_loss=0.00120): 100%|██████████| 45/45 [00:06<00:00,  6.68it/s]\n",
      "Training (15409 / 62503 Steps) (L2_loss=0.00139): 100%|██████████| 45/45 [00:06<00:00,  6.98it/s]\n",
      "Training (15454 / 62503 Steps) (L2_loss=0.00114): 100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
      "Training (15499 / 62503 Steps) (L2_loss=0.00473): 100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
      "Training (15544 / 62503 Steps) (L2_loss=0.00428): 100%|██████████| 45/45 [00:06<00:00,  6.61it/s]\n",
      "Training (15589 / 62503 Steps) (L2_loss=0.00398): 100%|██████████| 45/45 [00:06<00:00,  6.54it/s]\n",
      "Training (15634 / 62503 Steps) (L2_loss=0.00264): 100%|██████████| 45/45 [00:06<00:00,  6.48it/s]\n",
      "Training (15679 / 62503 Steps) (L2_loss=0.00128): 100%|██████████| 45/45 [00:06<00:00,  6.52it/s]\n",
      "Training (15724 / 62503 Steps) (L2_loss=0.00296): 100%|██████████| 45/45 [00:06<00:00,  6.46it/s]\n",
      "Training (15769 / 62503 Steps) (L2_loss=0.00065): 100%|██████████| 45/45 [00:06<00:00,  6.44it/s]\n",
      "Training (15814 / 62503 Steps) (L2_loss=0.00414): 100%|██████████| 45/45 [00:07<00:00,  6.40it/s]\n",
      "Training (15859 / 62503 Steps) (L2_loss=0.00301): 100%|██████████| 45/45 [00:06<00:00,  6.47it/s]\n",
      "Training (15904 / 62503 Steps) (L2_loss=0.00156): 100%|██████████| 45/45 [00:07<00:00,  6.27it/s]\n",
      "Training (15949 / 62503 Steps) (L2_loss=0.00261): 100%|██████████| 45/45 [00:07<00:00,  5.96it/s]\n",
      "Training (15994 / 62503 Steps) (L2_loss=0.00128): 100%|██████████| 45/45 [00:06<00:00,  6.44it/s]\n",
      "Training (16039 / 62503 Steps) (L2_loss=0.00130): 100%|██████████| 45/45 [00:06<00:00,  6.56it/s]\n",
      "Training (16084 / 62503 Steps) (L2_loss=0.00111): 100%|██████████| 45/45 [00:06<00:00,  6.85it/s]\n",
      "Training (16129 / 62503 Steps) (L2_loss=0.00355): 100%|██████████| 45/45 [00:06<00:00,  6.65it/s]\n",
      "Training (16174 / 62503 Steps) (L2_loss=0.00389): 100%|██████████| 45/45 [00:06<00:00,  6.72it/s]\n",
      "Training (16219 / 62503 Steps) (L2_loss=0.00124): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (16264 / 62503 Steps) (L2_loss=0.00133): 100%|██████████| 45/45 [00:08<00:00,  5.18it/s]\n",
      "Training (16309 / 62503 Steps) (L2_loss=0.00136): 100%|██████████| 45/45 [00:09<00:00,  4.76it/s]\n",
      "Training (16354 / 62503 Steps) (L2_loss=0.00134): 100%|██████████| 45/45 [00:06<00:00,  6.93it/s]\n",
      "Training (16399 / 62503 Steps) (L2_loss=0.00140): 100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
      "Training (16444 / 62503 Steps) (L2_loss=0.00237): 100%|██████████| 45/45 [00:06<00:00,  7.00it/s]\n",
      "Training (16489 / 62503 Steps) (L2_loss=0.00473): 100%|██████████| 45/45 [00:06<00:00,  6.99it/s]\n",
      "Training (16534 / 62503 Steps) (L2_loss=0.00128): 100%|██████████| 45/45 [00:06<00:00,  6.77it/s]\n",
      "Training (16579 / 62503 Steps) (L2_loss=0.00191): 100%|██████████| 45/45 [00:06<00:00,  6.61it/s]\n",
      "Training (16624 / 62503 Steps) (L2_loss=0.00270): 100%|██████████| 45/45 [00:06<00:00,  6.84it/s]\n",
      "Training (16669 / 62503 Steps) (L2_loss=0.00120): 100%|██████████| 45/45 [00:06<00:00,  6.98it/s]\n",
      "Training (16714 / 62503 Steps) (L2_loss=0.00280): 100%|██████████| 45/45 [00:06<00:00,  7.01it/s]\n",
      "Training (16759 / 62503 Steps) (L2_loss=0.00402): 100%|██████████| 45/45 [00:06<00:00,  7.01it/s]\n",
      "Training (16804 / 62503 Steps) (L2_loss=0.00171): 100%|██████████| 45/45 [00:06<00:00,  7.02it/s]\n",
      "Training (16849 / 62503 Steps) (L2_loss=0.00158): 100%|██████████| 45/45 [00:07<00:00,  6.26it/s]\n",
      "Training (16894 / 62503 Steps) (L2_loss=0.00317): 100%|██████████| 45/45 [00:07<00:00,  6.09it/s]\n",
      "Training (16939 / 62503 Steps) (L2_loss=0.00257): 100%|██████████| 45/45 [00:06<00:00,  7.01it/s]\n",
      "Training (16984 / 62503 Steps) (L2_loss=0.00110): 100%|██████████| 45/45 [00:06<00:00,  7.13it/s]\n",
      "Training (17029 / 62503 Steps) (L2_loss=0.00258): 100%|██████████| 45/45 [00:06<00:00,  7.12it/s]\n",
      "Training (17074 / 62503 Steps) (L2_loss=0.00222): 100%|██████████| 45/45 [00:06<00:00,  6.68it/s]\n",
      "Training (17119 / 62503 Steps) (L2_loss=0.00314): 100%|██████████| 45/45 [00:06<00:00,  6.78it/s]\n",
      "Training (17164 / 62503 Steps) (L2_loss=0.00109): 100%|██████████| 45/45 [00:06<00:00,  7.06it/s]\n",
      "Training (17209 / 62503 Steps) (L2_loss=0.00281): 100%|██████████| 45/45 [00:06<00:00,  6.75it/s]\n",
      "Training (17254 / 62503 Steps) (L2_loss=0.00162): 100%|██████████| 45/45 [00:06<00:00,  7.10it/s]\n",
      "Training (17299 / 62503 Steps) (L2_loss=0.00221): 100%|██████████| 45/45 [00:06<00:00,  6.78it/s]\n",
      "Training (17344 / 62503 Steps) (L2_loss=0.00143): 100%|██████████| 45/45 [00:06<00:00,  6.69it/s]\n",
      "Training (17347 / 62503 Steps) (L2_loss=0.00061):   7%|▋         | 3/45 [00:00<00:06,  6.62it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "oar_run_train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fold 0 \n",
    "arytenoid 0.64386\n",
    "a_carotid_l 0.83728 (아마이것보단 높을것)\n",
    "a_carotid_r 0.88483\n",
    "bone_mandible 0.964496\n",
    "oralcavity 0.924\n",
    "cochlea_l 0.85533\n",
    "optnr_1 0.77\n",
    "\n",
    "fold 1 \n",
    "arytenoid 0.45... 가우시안 노이즈 + 가우시안 블러링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swinjupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
